# -*- coding: utf-8 -*-
"""Danny_Steiner_Capstone_Full_Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19xQMXZbR5eZP9NnED0G1N06d_FR1d_Fp

# **Marketing Campaign Analysis**

## **Problem Definition**

### **The Context:**

 - Why is this problem important to solve?

This is important to solve as we are trying to help our client maximize their ROI on marketing. By showing them the best segmentation of their customer base they will be able to accurately target each customer with advertising that they are more likely to engage with, thus drawing more traffic to their website or storefront and increasing overall revenue.

### **The objective:**

 - What is the intended goal?

The intended goal is to take the dataset we are given and create the best possible customer segmentations we are able to. We will then give this information to our client in hopes that they will be able to better understand their customer segment and better target each customer with advertising.

### **The key questions:**

- What are the key questions that need to be answered?

The key questions needing to be answered are, what variables are most important in understanding each customer segment? What variables are not? What is the most accurate number of clusters we can create that will help our client better target their customers?

### **The problem formulation**:

- What is it that we are trying to solve using data science?

The problem we are trying to solve is how can we reduce 27 variables down to a core few and still accurately segment our client’s customer base into clusters that will help them target each buyer with marketing that will convert them into buyers.

------------------------------
## **Data Dictionary**
------------------------------

The dataset contains the following features:

1. ID: Unique ID of each customer
2. Year_Birth: Customer’s year of birth
3. Education: Customer's level of education
4. Marital_Status: Customer's marital status
5. Kidhome: Number of small children in customer's household
6. Teenhome: Number of teenagers in customer's household
7. Income: Customer's yearly household income in USD
8. Recency: Number of days since the last purchase
9. Dt_Customer: Date of customer's enrollment with the company
10. MntFishProducts: The amount spent on fish products in the last 2 years
11. MntMeatProducts: The amount spent on meat products in the last 2 years
12. MntFruits: The amount spent on fruits products in the last 2 years
13. MntSweetProducts: Amount spent on sweet products in the last 2 years
14. MntWines: The amount spent on wine products in the last 2 years
15. MntGoldProds: The amount spent on gold products in the last 2 years
16. NumDealsPurchases: Number of purchases made with discount
17. NumCatalogPurchases: Number of purchases made using a catalog (buying goods to be shipped through the mail)
18. NumStorePurchases: Number of purchases made directly in stores
19. NumWebPurchases: Number of purchases made through the company's website
20. NumWebVisitsMonth: Number of visits to the company's website in the last month
21. AcceptedCmp1: 1 if customer accepted the offer in the first campaign, 0 otherwise
22. AcceptedCmp2: 1 if customer accepted the offer in the second campaign, 0 otherwise
23. AcceptedCmp3: 1 if customer accepted the offer in the third campaign, 0 otherwise
24. AcceptedCmp4: 1 if customer accepted the offer in the fourth campaign, 0 otherwise
25. AcceptedCmp5: 1 if customer accepted the offer in the fifth campaign, 0 otherwise
26. Response: 1 if customer accepted the offer in the last campaign, 0 otherwise
27. Complain: 1 If the customer complained in the last 2 years, 0 otherwise

**Note:** You can assume that the data is collected in the year 2016.

## **Import the necessary libraries and load the data**
"""

pip install scikit-learn-extra #Hard install scikit since Colab didn't have sklearn_extra for kmeans

# Libraries for reading and manipulating data
import numpy as np
import pandas as pd

# Libraries for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# library to scale data
from sklearn.preprocessing import StandardScaler

# library to compute distances
from scipy.spatial.distance import cdist

# library to run K-means clustering and compute Silhouette scores
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# library to visualize the elbow curve and Silhouette scores
from yellowbrick.cluster import SilhouetteVisualizer

# Importing PCA
from sklearn.decomposition import PCA

# library to encode the variables
from sklearn.preprocessing import LabelEncoder

# Importing TSNE
from sklearn.manifold import TSNE

# library to run hierarchical clustering
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

# library to compute distances
from scipy.spatial.distance import pdist

# library to run K-Medoids
from sklearn_extra.cluster import KMedoids

# library to run Gaussian Mixture
from sklearn.mixture import GaussianMixture

# Supress warnings
import warnings

# Library to run DBSCAN
from sklearn.cluster import DBSCAN

warnings.filterwarnings("ignore")

"""## **Data Overview**

- Reading the dataset
- Understanding the shape of the dataset
- Checking the data types
- Checking for missing values
- Checking for duplicated values
- Drop the column which has no null values
"""

# connecting google drive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv(r"/content/drive/MyDrive/Data Science/Captstone/marketing_campaign+%284%29.csv") #reading the data
df.head()

df.shape #Checking the shape of the data frame

df.info() #looking at the overall info of the dataframe and checking data type

df.isnull().sum() #checking for any null values

df[df.duplicated()] #checking for duplicated values

"""#### **Observations and Insights from the Data overview:**

The data set has 27 columns and 2,240 rows. All elements are integers with the exception of Education, Marital Status, and DT Customer key which are objects, and income which is float. The only column with null values is income which has 24. I will most likely impute these with the median income value depending on the distribution of the data. Finally, it looks like we don’t have any duplicated values in the dataset.

## **Exploratory Data Analysis (EDA)**

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Questions:**

1. What is the summary statistics of the data? Explore summary statistics for numerical variables and the categorical variables

2. Find out number of unique observations in each category of categorical columns? Write your findings/observations/insights

3. Are all categories different from each other or can we combine some categories? Is 2n Cycle different from Master?

4. There are 8 categories in Marital_Status with some categories having very low count of less than 5. Can we combine these categories with other categories?

**Observations**

- The mean birth year is 1968, if we assume the data was collected in 2016 that would mean that our mean age is 48. The median age is only two years later in 1970 meaning the median age is 50. The standard deviation is 11 years meaning that we have a very small spread.

- The mean income is 52,247.25 and the median income is 51,381.50 Some of the elements in the income column are missing, so I will most likely impute these with the median income.

- Not many households seem to have young children or teenagers, but judging by the mean, it seems as though the number of houses with teenagers has a slightly higher count.

- The mean and median number of days since a customer's last purchase is 49 days with a standard deviation of 28 days. This tells me that this is more of a speciality food store and not something like a grocery store where consumers visit it weekly.

- Out of all the products they spend money on, they spent the most amount on wine with a mean of 303.93 and a median of 173.50. The data has an extremely widespread at 336.59. This tells me that this category most likely has a bunch of outliers in it that is creating a massive spread and a wide disparity between mean and the median. Otherwise, meat products have the second highest spend and fruits, sweets, and fish all filter in behind with comparable numbers. All these products display high means with low medians and high standard deviations. That tells me that all of them have a ton of outliers that are adding noise to the data.

- Overall it looks like the store receives more purchases than the catalog. The number of website visits column is slightly comparable to the instore purchases column. This tells me that this store may be struggling to break out of its current brick and mortar market into other markets.

- The most accepted web campaign was campaign 4, the least accepted campaign was campaign 1. I’m not sure what determined this, but I’m excited to see how this influences clusters as we move on to using different models.

- Finally, it doesn’t look like this company has received too many complaints from their customers which is positive. I’m wondering if we may be able to drop this column since the data is so low and doesn’t give us a lot.

- ID has 2240 unique values, we can drop this as it does’t offer any information for our analysis. Education has 5 unique values and marital status has 8. This is because there are some elements in here that may mean the same thing in both columns, so we’ll have to combine them.

- Not all of the categories are different. I combined 2n cycle with masters, otherwise I left the rest untouched.

- We can also combine some of the catedories in the marital status column into single. I will combine Yolo, Absurd and Alone into single to streamline our analysis.
"""

df.nunique() #checking all unique values

df.drop(columns = ["ID"], inplace = True) #dropping the ID variable since this won't tell us much about each customer and it has 2240 unqiue values

df.rename(columns = {'Response':'AcceptedCmp6'}, inplace = True) #Renaming 'Response' column to 'AcceptedCmp6' for more continuity in name

df.tail() #checking to make sure the ID column was dropped and Response was renamed to 'AcceptedCmp6'

df = df.replace("Alone", np.nan) #changing values "Alone", "Absurd", and "YOLO" to single since these are most likely single people. In the next cell we will change these elements to "Single"
df = df.replace("Absurd", np.nan)
df = df.replace("YOLO", np.nan)

df['Marital_Status'].fillna(value = 'Single', inplace = True) #replacing NaN values with 'Single'

df = df.replace("2n Cycle", np.nan) #changing value '2n cycle to NaN to be switched to 'Master'

df['Education'].fillna(value = 'Master', inplace = True) #replacing NaN values with 'Master'

df.describe() #pulling the summary statistics for each column

"""### **Univariate Analysis on Numerical and Categorical data**

Univariate analysis is used to explore each variable in a data set, separately. It looks at the range of values, as well as the central tendency of the values. It can be done for both numerical and categorical variables.

- Plot histogram and box plot for different numerical features and understand how the data looks like.
- Explore the categorical variables like Education, Kidhome, Teenhome, Complain.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Leading Questions**:
1. How does the distribution of Income variable vary across the dataset?
2. The histogram and the box plot are showing some extreme value on the right side of the distribution of the 'Income' feature. Can we consider them as outliers and remove or should we analyze these extreme values?
3. There are only a few rows with extreme values for the Income variable. Is that enough information to treat (or not to treat) them? At what percentile the upper whisker lies?

**Observations**

- How does the distribution of Income variable vary across the dataset?

Income is very heavily positively skewed at 6.76. This is odd considering that income distribution is usually normal. In this case it looks like there’s a cluster around the mean and a lot of outliers in the data.

- The histogram and the box plot are showing some extreme value on the right side of the distribution of the 'Income' feature. Can we consider them as outliers and remove or should we analyze these extreme values?

I would say we should analyze these extreme values. We have such a small dataset already that if we were to get rid of any of the rows we would risk further diminishing the refinement of our models.


- There are only a few rows with extreme values for the Income variable. Is that enough information to treat (or not to treat) them? At what percentile the upper whisker lies?

That isn’t sufficient information to treat them. The upper whisker looks like it lies somewhere in the 99th percentile.

- Further Observations

Birth years seem to be the only variable that are evenly distributed with a heavy emphasis around the 1970s. This tells us that our population is primarily middle aged around the collection of this data.

The vast majority of these households either have no children or one child. This is interesting considering that we are examining almost no large families. Given that our median income is around 53,000 we’re most likely looking at middle income families.

Recency looks to be uniformly distributed. This makes sense given that each person’s interaction with the store might not follow a particular trend, but will be influenced by when they run out of a particular product or when they might be able to afford a particular product.

The wines, fruits, meat, fish, sweet, and gold products all have a skewed distribution with a skew of around 1.8-2.1. This is interesting because the distribution mirrors the income distribution, and we don’t necessarily see one product being favored over another. I imagine that the more expensive products are being purchased more by the wealthy clients and the less expensive products are being purchased more by the lower income clients. There are some products that have an edge over others, like fruit and wine products, but for the most part, all of them have a positive skew. I imagine that this is due to the fact that since the data collection reflects the amount of products purchased by a client within the past two years, we’ll see more outliers as some clients shop with the store more than others over the past two years.

Number of deals purchased has a heavy right skew, with a steep drop after 1. This makes sense given that the average consumer is most likely not going to be using coupons all the time, but every once in a while.

Number of web purchases has a positive skew with an interesting bump around 4. This store doesn’t seem to have a huge web presence judging by how low the numbers in this column are. It seems like the numbers start lessening after 10, so I don’t imagine this is a big part of their revenue.

Number of catalog purchases is even smaller than web purchases. This is most likely due to how outdated catalogs are and the fact that very few people would actually order out of them.

Instore purchases have a much more uneven distribution, but they seem to have higher numbers. This tells me that this is most likely a small business that does a good amount of their business in their store.

Number or web visits almost has a normal distribution however there is a big drop after 10 for some reason. This tells me that their website doesn’t get a lot of web traffic. This furthers my theory that this is a small business that does a lot of their revenue primarily in-store.

The rest of the variables don’t have a noticeable distribution as they represent whether or not offers have been accepted so it’s either a 1 or a 0.
"""

result = df.select_dtypes(include = 'number') # pulling columns from the dataframe with numbers

cont_cols = list(result) #listing all columns with numbers

for col in cont_cols: #setting for loop to iterate over all columns and output histograms

    print(col) #print all columns

    print('Skew :',round(df[col].skew(),2)) #print skew of each graph

    plt.figure(figsize = (15, 4)) #figure size

    plt.subplot(1, 2, 1) #subplot size

    df[col].hist(bins = 10, grid = False)  #code for histogram with 10 bins

    plt.ylabel('count') #labeling y axis

    plt.subplot(1, 2, 2) #creating subplots

    sns.boxplot(x = df[col]) #creating boxplots for all columns

    plt.show() #show the boxplots and histograms

"""**Bivariate Analysis**

- Analyze different categorical and numerical variables and check how different variables are related to each other.
- Check the relationship of numerical variables with categorical variables.
"""

plt.figure(figsize = (18, 12)) #setting figure size
sns.heatmap(df.corr(), annot = True) #creating correlation matrix and passing it as an argument through the heatmap function
plt.show() #show the correlation matrix

"""**Observations**

- To begin, we see a mildly strong negative correlation between teen home and year birth. This is most likely because if a client has kids, then the older they are the older their kids will be. I.e. the lesser the date, the higher the number of kids.

- Next we see stronger correlations in the income column. There are more negative correlations between income and web visits and income and kid home. The web visits may be due to the fact that lower income folks may be forced to shop online as they may not be able to shop in store as frequently. Kid in home may be negatively correlated because the cost of raising children can have a drain on overall income. Finally, we see all positive correlations between the products purchased and how they’re purchased. This most likely mirrors income because higher income folks are generally going to buy more products.

- Interestingly enough, the kid home column has a negative correlation with the products purchased and how they’re purchased columns. I imagine this is because folks are going to have to cut back on spending the more kids they have to save money.

- Another mild positive correlation we see is the number of deals purchased and the number of teens in home. I imagine this is due to the fact that teenagers eat more than kids, and parents are going to be looking for ways to cut down on spending while still feeding their families.

- Finally, the number of web visits per month have negative correlations with all product purchased categories. I imagine this has something to do with the fact that not many clients buy products from the website, but may intend to use it just to browse. So, as visits increase it may not necessarily indicate that purchases will follow if clients are just using it to view the inventory.

### **Feature Engineering and Data Processing**

In this section, we will first prepare our dataset for analysis.
- Imputing missing values

**Think About It:**

- Can we extract the age of each customer and create a new feature?
- Can we find the total kids and teens in the home?
- Can we find out how many members each family has?
- Can we find the total amount spent by the customers on various products?
- Can we find out how long the customer has been with the company?
- Can we find out how many offers the customers have accepted?
- Can we find out amount spent per purchase?
"""

df['Income'] = df['Income'].fillna(df['Income'].median()) #imputing NaN values in income with the median value in the column

df['Age'] = 2016 - df['Year_Birth'] #Creating age column by subtracting the year the data was collected by the Year birth column

df.drop(columns = ["Year_Birth"], inplace = True) #Dropping the Year_Birth column since we now have the age column

#changing values married and together to partnered and widowed and divorced to single
df = df.replace("Married", "Partnered")
df = df.replace("Together", "Partnered")
df = df.replace("Widow", "Single")
df = df.replace("Divorced", "Single")

df = df.replace("Single", 1) #Replacing single with 1 and partner with 2 to create family number variable
df = df.replace("Partnered", 2)

df['Total_kids_in_home'] = df['Kidhome'] + df['Teenhome'] #Creating a new column in the data frame that sums the number of children and teens in the home

df.drop(columns = ['Kidhome'], inplace = True) #dropping Kidhome column

df.drop(columns = ['Teenhome'], inplace = True) #dropping Teenhome column

df["Family_Size"] = df["Marital_Status"] + df["Total_kids_in_home"] #combining marital status and total kids in home to create a variable called family size

df.drop(columns = 'Marital_Status', inplace = True) #dropping Marital_Status since we have made the new feature "Family size"

import datetime as DT #importing datetime to convert Dt_Customer columns to date and time
df['Dt_Customer']= pd.to_datetime(df['Dt_Customer']) #converting Dt_customer to datetime

Lenth_of_Support = df["Dt_Customer"].apply(lambda x : (2016 - x.year)) #creating new dataframe to calculate the length of time a customer has been with the company
#based off of the date they joined minus 2016

Lenth_of_Support = Lenth_of_Support.rename('Length_of_Support') #Naming the dataframe 'Lenth_of_Support' since it originally defaulted to "Year_Birth"

df = pd.concat([df, Lenth_of_Support], axis=1) #concatenating the two dataframes to create a new feature

df.drop(columns = "Dt_Customer", inplace = True) #dropping "Dt_Customer" column since we created a new feature

df['Offers_Accepted'] = df['AcceptedCmp1'] + df['AcceptedCmp2'] + df['AcceptedCmp3'] + df['AcceptedCmp4'] + df['AcceptedCmp5'] + df['AcceptedCmp6']
#summing accepted columns 1-6 to create a new column called "Offers accepted" the reflects the total offers accepted of each customer

df.drop(columns = ["AcceptedCmp1"], inplace = True) #dropping all "accepted columns"

df.drop(columns = ["AcceptedCmp2"], inplace = True)
df.drop(columns = ["AcceptedCmp3"], inplace = True)
df.drop(columns = ["AcceptedCmp4"], inplace = True)
df.drop(columns = ["AcceptedCmp5"], inplace = True)
df.drop(columns = ["AcceptedCmp6"], inplace = True)

df['Amount_spent_per_purchase'] = df['MntWines'] + df['MntMeatProducts'] + df['MntFishProducts'] + df['MntSweetProducts'] + df['MntGoldProds'] + df['MntFruits']
#creating new column for amount spent per purchase

df.drop(columns = ["MntWines"], inplace = True) #dropping all product columns since we have a column showing how much was spent on each purchase
df.drop(columns = ["MntMeatProducts"], inplace = True)
df.drop(columns = ["MntFishProducts"], inplace = True)
df.drop(columns = ["MntSweetProducts"], inplace = True)
df.drop(columns = ["MntGoldProds"], inplace = True)
df.drop(columns = ["MntFruits"], inplace = True)

# Adding the number of purchases from each channel 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases' to create new feature called "Number_of_purchases"
df["Number_of_purchases"] = df['NumDealsPurchases'] + df['NumWebPurchases'] + df['NumCatalogPurchases'] + df['NumStorePurchases']

df.drop(columns = 'NumDealsPurchases', inplace = True)
df.drop(columns = 'NumCatalogPurchases', inplace = True)
df.drop(columns = 'NumWebPurchases', inplace = True)
df.drop(columns = 'NumStorePurchases', inplace = True)
df.drop(columns = 'NumWebVisitsMonth', inplace = True)

df #checking dataframe to ensure all new columns were added and all old columns were dropped

"""## **Important Insights from EDA and Data Preprocessing**

What are the the most important observations and insights from the data based on the EDA and Data Preprocessing performed?

## **Data Preparation for Segmentation**

- The decision about which variables to use for clustering is a critically important decision that will have a big impact on the clustering solution. So we need to think carefully about the variables we will choose for clustering. Clearly, this is a step where a lot of contextual knowledge, creativity, and experimentation/iterations are needed.
- Moreover, we often use only a few of the data attributes for segmentation (the segmentation attributes) and use some of the remaining ones (the profiling attributes) only to profile the clusters. For example, in market research and market segmentation, we can use behavioral data for segmentation (to segment the customers based on their behavior like amount spent, units bought, etc.), and then use both demographic as well as behavioral data for profiling the segments found.
- Plot the correlation plot after we've removed the irrelevant variables
- Scale the Data

**Observations**

After some trial and error I decided to keep the columns Income, Recency, Age, Total kids in home, length of support, offers accepted, and amount spent per purchase. These variables represented a summation many of the smaller columns such as product purchases, and kids vs. teens in homes. Additionally, this represented the best spread of behavioral and demographic variables to utilize for the models.
"""

df.drop(columns = 'Education', inplace = True) #dropping Education since this is an object and cannot be scaled

df.drop(columns = 'Complain', inplace = True) #dropping complain since this does not give us much information about each individual customer

plt.figure(figsize = (6, 6)) #setting figure size
sns.heatmap(df.corr(), annot = True) #creating correlation matrix and passing it as an argument through the heatmap function
plt.show()

"""**Observations**

Starting in the income column we see a mild negative correlation with total kids in home. This is most likely due to the fact that as the number of kids in the home increases overall income may decrease due to the cost of raising a child. We see a positive correlation with age and income. This can be attributed to the fact that the older you get, the more money you make due to having more success in your career. Interestingly enough, we see a higher correlation between offers accepted and income. This is most likely attributed to the fact that higher income individuals may have more access to laptops and phones where they can have offers sent to them. Finally, unsurprisingly, we see a strong correlation between income and amount spent per purchase. This is in part due to the fact that the more money someone has, the more money they’ll spend on products.

Recency doesn’t seem to have any strong correlations across any variables. This was a similar situation in the original correlation matrix. I imagine this is due to the fact that how often someone visits the store is going to be based on necessity and not necessarily behavior or demographics.

Age also doesn’t seem to have a high correlation between any variable other than age. This I found particularly odd as I would assume that given the correlation between age and income we would at least see a stronger correlation between age and amount spent per purchase.

Total kids in home has a negative correlation with amount spent per purchase. This is probably due to the fact that as the number of kids in the home increases parents are looking to cut costs where they can. This tells me that we may be dealing with a specialty store and not a grocery store. Parents are more likely to spend more money at a grocery store where they can buy food at a cheaper price in bulk than at a speciality store where they buy food at a more expensive price in limited quantities. Total kids in home is oddly slightly negatively correlated with offers accepted. This may be because parents don’t have the time to monitor all the offers that are being sent to them let alone use them.

Finally, in length of support we see a slight positive correlation with amount spent per purchase. This can be explained by the fact that the longer someone stays a customer the more they’re likely to spend on products because they enjoy the products the store sells.

"""

scaler = StandardScaler() #Initializing standard scaler

data_scaled = pd.DataFrame(scaler.fit_transform(df), columns = df.columns) #scaling the data

data_scaled.head() #viewing data afer scaling

"""## **Applying T-SNE and PCA to the data to visualize the data distributed in 2 dimensions**

### **Applying T-SNE**
"""

tsne = TSNE(n_components = 3, random_state = 1) # Applying the t-SNE algorithm using 3 components. I ran this with 2 and the clusters immediately melded together. 3 yielded the best results with the tightest clusters
data_tsne = tsne.fit_transform(data_scaled)    # Fitting and transforming the t-SNE function on the scaled data

data_tsne = pd.DataFrame(data = data_tsne, columns = ['Component 1', 'Component 2', 'Component 3']) #adding the components to the tsne data frame

data_tsne.head() #checking the data

sns.scatterplot(x = data_tsne.iloc[:,0], y = data_tsne.iloc[:,1]) #visualizing the data

plt.show()

"""**Observation and Insights:**

I tried a few different number of components, and found that 3 gave me the best results. Even though the cluster on the right is still pretty far apart, I can see roughly 5 clusters develop from using just three components. Some of them are pretty tight, some of them are still pretty far apart and need to be brought together more. This tells me that I either needed another variable in my final dataset, or perhaps there was something off with my scaling. When I tried 2 components the clusters immediately blew up, so 3 seemed to be the best here

### **Applying PCA**
"""

n = data_scaled.shape[1] # Defining the number of principal components


pca = PCA(n_components = n, random_state = 1)  # Applying PCA algorithm

data_pca1 = pd.DataFrame(pca.fit_transform(data_scaled))   # Fit and transform the pca function on scaled data

exp_var = pca.explained_variance_ratio_ # The percentage of variance explained by each principal component

plt.figure(figsize = (10, 10)) #Figure size

plt.plot(range(1, 10), exp_var.cumsum(), marker = 'o', linestyle = '--') #creating the plot using cumulative sum and exponential variance

plt.title("Explained Variances by Components") #adding title

plt.xlabel("Number of Components") #adding x label

plt.ylabel("Cumulative Explained Variance") #adding y label

plt.show() #displaying plot

sum = 0 #setting sum = 0 for for loop below

for ix, i in enumerate(exp_var): #creating for loop to tell me what number of PCs explaing 80% of variance

    sum = sum + i

    if(sum>0.80): #If the sum greater than 0 return print statement below.
        print("Number of PCs that explain at least 80% variance: ", ix + 1)
        break

"""**Think about it:**
- Should we apply clustering algorithms on the current data or should we apply PCA on the data before applying clustering algorithms? How would this help?

We should apply PCA on the dataset before applying any clustering algorithms. PCA allows us to reduce the dataset down to just a few principal components which is imperative since the goal of clustering algorithms is to get the tightest clusters. The more variables we have the harder this gets. Additionally, with some algorithms like k-means, computation can be time consuming and expensive with larger datasets, so it’s more beneficial to only have a few principal components for clustering.

**Observation and Insights:**

After applying PCA we see that 80% of the data's variance is explained with just 5 components. This is almost a 50% reduction in components that still yields an 80% explanation in variance. That's quite impressive! That means we can still get rid of almost half of our variables and still account for 80% of our data.

## **K-Means**
"""

sse = {} #creating a dictionary to store all the SSEs

for k in range(1, 10): # using a for loop to iterate for a range of Ks then fitting the pca components to the algorithm.
    kmeans = KMeans(n_clusters = k, max_iter = 1000, random_state = 1).fit(data_scaled)
    sse[k] = kmeans.inertia_

plt.figure() #visualizing each cluster

plt.plot(list(sse.keys()), list(sse.values()), 'bx-') #plotting all keys

plt.xlabel("Number of cluster") #adding x label

plt.ylabel("SSE") #adding y label

plt.show() #displaying plot

from sklearn.metrics import silhouette_score #importing silhoutette_score from sklearn since the elbow method didn't give us a clear number of clusters to use

sc = {} # Creating an empty dictionary to store the Silhouette score for each value of K

for k in range(2, 10): # Creating a for loop to iterate for a range of Ks then fitting the scaled data to the algorithm.
    kmeans = KMeans(n_clusters = k, random_state = 1).fit(data_scaled)

    labels = kmeans.predict(data_scaled)

    sc[k] = silhouette_score(data_scaled, labels)


plt.figure() #visualizing plot

plt.plot(list(sc.keys()), list(sc.values()), 'bx-') #plotting all keys

plt.xlabel("Number of cluster") #adding x title

plt.ylabel("Silhouette Score") #adding y title

plt.show() #displaying plot

"""**Think About It:**

- How do we determine the optimal K value from the elbow curve?

There are two ways we can do this, we can use the "elbow" method in which we plot the within cluster sum of squares and look for the point in the graph in which the reduction of WCSS is no longer significant. This creates an elbow effect that suggests the optimal k-value. We can also find this by calculating and plotting the silhouette scores or the distance between a sample and the nearest cluster that the sample is not a part of. Whereas with WCSS we're looking for an elbow, with silhouette scores we're looking for the highest point on the graph.

- Which metric can be used to determine the final K value?

Both can be used to determine the final k-value. However, sometimes one can be more clear than the other. In this case, I chose to do both, because WCSS didn't provide a clear enough elbow. However, after calculating and plotting the silhouette scores, I found that the optimal k value in this instance is 2, which I decided to use.

### **Applying KMeans on the PCA data and visualize the clusters**
"""

kmeans = KMeans(n_clusters = 2, random_state = 1) # Using the K-Means algorithm with 2 clusters and random_state=1

kmeans.fit(data_pca1.values) # Fitting on the pca data

df["K_means_labels"] = kmeans.labels_ # Adding K-Means cluster labels to the data

"""### **Cluster Profiling**"""

plt.figure(figsize = (30, 50)) #setting figure size

for i, variable in enumerate(df): #creating for loop to iterate through all items in the dataframe
    plt.subplot(8, 3, i + 1)

    sns.boxplot(y=df[variable], x=df["K_means_labels"]) #creting the box plots

    plt.tight_layout() #setting the layout

    plt.title(variable) #adding title

plt.show()

"""**Observations and Insights:**

The profiles seemed to have clustered customers into two lower class incomes. I know that this isn’t representative of our data, so I’m going to try to re-run the algorithm with k=5 below.
"""

df.drop(columns = ["K_means_labels"], inplace = True) #dropping the k means labels prediction column to use the original PCA data for prediction

kmeans_2 = KMeans(n_clusters = 5, random_state = 1) # Using the K-Means algorithm with 5 clusters and random_state=1

kmeans_2.fit(data_pca1) # Fitting on the pca data

df["K_means_labels"] = kmeans_2.labels_  # Adding K-Means cluster labels to the data

plt.figure(figsize = (30, 50))

for i, variable in enumerate(df):
    plt.subplot(8, 3, i + 1)

    sns.boxplot(y=df[variable], x=df["K_means_labels"])

    plt.tight_layout()

    plt.title(variable)

plt.show()

"""### **Characteristics and Summary of Each Cluster**

- Cluster 0: This cluster is more upper class. They have a mid-frequency interaction with the store. Their age varies greatly. They are likely to have 1-2 kids and as a result have larger families. They are fierce loyalists of the storey and have been supporters for a while. They’ll normally accept offers, and as a result spend the third most per purchase. Finally, they may not spend the most per purchase, but they do make the most purchases out of all the other clusters.

- Cluster 1: This cluster is lower class. They shop at the store about as much as a cluster 0. This cluster is younger. This cluster has 1-2 kids, but on average may not have a large family. This cluster is a fairly new supporter of the store as the max customer has been around for 4 years. This cluster doesn’t accept many offers on average. They spend the least amount of money per purchase compared to the rest of the clusters. Finally, they make the least amount of purchases out of all the clusters.

- Cluster 2: This cluster is more affluent with their median income being around $80,000 however, there are a lot more outliers in this data. They shop at the store more than any other cluster with a score closer to 60. This cluster is older, but also has the greatest variance in age as we see the box plot spans the largest age range. This cluster is unlikely to have kids and as a result doesn’t have large families. This cluster is a fairly new supporter of the store as the max customer has been around for 4 years. This cluster accepts more offers on average than the other clusters. They spend the second most amount of money per purchase, but they also have quite a bit of variance in how much they spend. Finally, they make the second most amount of purchases out of all the other clusters with a median of 20.

- Cluster 3: This cluster is primarily middle class as they sit between cluster 0 and cluster 1. They shop at the store almost as much as cluster 2 does. This cluster is older with a median age of 50 and a max age of 70. This cluster has 2 kids on average and as a result has much larger families. This cluster is a fairly new supporter of the store as the max customer has been around for 4 years. This cluster doesn’t accept many offers at all. They spend a little more than cluster 1 per purchase, but not much. Finally, they out purchase cluster 1 but only by a small margin in gross numbers.

- Cluster 4: This cluster is very affluent with their median income being around 90,000 and their max being above 100,000. They shop at the store the least amount compared to the rest of the clusters. Like cluster 2, this cluster also has a wide age range, but they are slightly younger with a median age of 45. Like cluster 1, this cluster is unlikely to have kids and as a result has small families. This cluster is a fairly new supporter of the store as the max customer has been around for 4 years. They accept a lot of offers compared to the other clusters. They spend the most amount of money per purchase out of all the clusters, but also have the greatest variance out of all the clusters. Finally, their purchasing is comparable to cluster 0 with a median of 22 purchases.

**Think About It:**
- Are the K-Means profiles providing any deep insights into customer purchasing behavior or which channels they are using?

Yes, the clusters are telling us that a majority of the individuals in the data are new customers, primarily in the middle to upper class range, don’t accept many offers, are generally likely to spend a lot per purchase, and will usually make a lot of purchases. So it seems like this store could be on the newer side since most of the customers are newer, but seem to be spending and purchasing a lot.

- What is the next step to get more meaningful insights?

The next step to getting meaningful insights would be to run more models. We cannot rely solely on one model to gain insights about our customer bases. Each model is going to have different outcomes, so we must choose the model with the best insights after running them all.

## **K-Medoids**
"""

df.drop(columns = 'K_means_labels', inplace = True)

kmedo = KMedoids(n_clusters = 5, random_state = 1) #initializing kmedoid algorithm with clusters at 5

kmedo.fit(data_scaled) #fitting kmedoid on the scaled data

df['kmedoLabels'] = kmedo.predict(data_scaled) #adding predictions to dataframe

"""### **Visualize the clusters using PCA**

### **Cluster Profiling**
"""

plt.figure(figsize = (30, 50))

for i, variable in enumerate(df):
    plt.subplot(8, 3, i + 1)

    sns.boxplot(y=df[variable], x=df["kmedoLabels"])

    plt.tight_layout()

    plt.title(variable)

plt.show()

"""### **Characteristics of each cluster**

**Summary for each cluster:**

- Cluster 0: This cluster is primarily lower income with a median around 30,000. The majority of them do not go to the store very often, however there are a few that do visit frequently. They have a pretty wide age range spanning from 20 all the way to 70, but their median age is around 45. They are likely to have a larger family, and haven’t been with the store for a while. They don’t accept many offers. They spend very little money per purchase, and out of all the clusters their total number of purchases are also pretty low.

- Cluster 1: This cluster is more affluent with their median income being around 70,000 and the upper end being close to 100,000. Their interaction with the store has an extremely wide spread, so it’s hard to tell how often these customers visit the store, but going off the median, it’s safe to say it’s around every 60 days. This cluster has a less vast age range and are generally more on the middle aged side. They may have 1 kid at home, otherwise they don’t have large families. They are fierce supporters of the store and have been with it for a while. They don’t accept many offers. They spend a pretty good amount per purchase which makes sense given the fact that they don’t have many kids and they frequently make a lot of purchases.

- Cluster 2: This cluster is slightly less affluent than cluster 1, but still within the same income range. They don’t purchase items from the store as often, but still more often than cluster 1. They are the oldest cluster out of all of them with the median age being 60 years old. They have almost no kids at home, but they still may have a large family. They are fierce supporters of the store and have been customers for quite some time. They frequently accept at least one offer. Their median purchase is around $600 however, the amount they spend per purchase varies greatly in both directions, and finally, they make purchases frequently with a median of purchase number of 23.

- Cluster 3: This is by far the most affluent cluster out of all 5, with a median income of $80,000 and one odd outlier at 600,000. They interact frequently with the store with a median total of 55 and an upper quartile range of around 75. They have the greatest range in age spanning from 20 to late 70s. They are highly unlikely to have kids and as a result do not have large families. They’re fairly new customers and haven’t been with the company that long, however they seem to accept the most amount of offers out of all the other clusters. They spend a great range of money per purchase, but it’s usually on the higher end. As a result, they also make a lot of purchases.

- Cluster 4: This cluster is about as affluent as cluster 0, however they have 1 outlier that seems to be very rich with an income close to 200,000. They shop at the store very often. They have a wide age range, but they’re primarily on the older side. They have many kids at home and as a result have large families. They are new to the store and as a result have rarely accepted an offer. They don’t spend a lot per purchase and they don’t make as many purchases either compared to other clusters.

**Observations and Insights:**

Comparing the K means and K medoids clusters, we see that the two are almost opposite to each other with a few exceptions. In cluster 0, the only similarities are age, family size, and amount spent per purchase. In cluster 1, we only see similarities in offers accepted. In cluster 2 we see similarities in age, family size, offers accepted, and amount of purchases. In cluster 3 we see similarities in length of support and number of purchases. Finally, in cluster 4 we only see a similarity in loyalty status. It’s interesting to see such contrasting results between the two. This is in part due to how differently both models calculate similarity measures, so I’m curious to see how the rest of the models compare.

## **Hierarchical Clustering**

- Find the Cophenetic correlation for different distances with different linkage methods.
- Create the dendrograms for different linkages
- Explore different linkages with each distance metric
"""

distance_metrics = ["euclidean", "chebyshev", "mahalanobis", "cityblock"] # creating list of distance

linkage_methods = ["single", "complete", "average"] # creating list of linkage methods

high_cophenet_corr = 0                                                 # Creating high_cophenet_corr var
high_dm_lm = [0, 0]                                                    # Creating high_dm_lm var

for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(data_pca1, metric=dm, method=lm)                    # Applying different linkages and distance on data_pca
        c, coph_dists = cophenet(Z, pdist(data_pca1))                   # Calculating cophenetic correlation
        print(
            "Cophenetic correlation for {} distance and {} linkage is {}.".format(
                dm.capitalize(), lm, c
            )
        )
        if high_cophenet_corr < c:                                     # Checking if cophenetic correlation is higher than previous score
            high_cophenet_corr = c                                     # Appending to high_cophenet_corr list if it is higher
            high_dm_lm[0] = dm                                         # Appending its corresponding distance
            high_dm_lm[1] = lm

print( # Printing the highest distance metric and linkage method with its cophenetic correlation
    "Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.".format(
        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]
    )
)

"""**Cityblock Distance**"""

methods = ['single', # The List of all linkage methods to check
           'average',
           'complete']


fig, axs = plt.subplots(len(methods), 1, figsize = (20, 15)) # Creating a subplot image


for i, method in enumerate(methods): # Iterating through the list of all methods above to pull linkage and plot dendrogram
    Z = linkage(data_scaled, metric = 'Cityblock', method = method)

    dendrogram(Z, ax = axs[i]);

    axs[i].set_title(f'Dendrogram ({method.capitalize()} Linkage)')

    axs[i].set_ylabel('Distance')

"""**Mahalanobis Distance**"""

methods = ['single',
           'average',
           'complete']

fig, axs = plt.subplots(len(methods), 1, figsize = (20, 15))

for i, method in enumerate(methods):
    Z = linkage(data_scaled, metric = 'Mahalanobis', method = method)

    dendrogram(Z, ax = axs[i]);

    axs[i].set_title(f'Dendrogram ({method.capitalize()} Linkage)')

    axs[i].set_ylabel('Distance')

"""**Chebyshev Distance**"""

methods = ['single',
           'average',
           'complete']

fig, axs = plt.subplots(len(methods), 1, figsize = (20, 15))

for i, method in enumerate(methods):
    Z = linkage(data_scaled, metric = 'Chebyshev', method = method)

    dendrogram(Z, ax = axs[i]);

    axs[i].set_title(f'Dendrogram ({method.capitalize()} Linkage)')

    axs[i].set_ylabel('Distance')

"""**Euclidean Distance**"""

methods = ['single',
           'average',
           'complete']

fig, axs = plt.subplots(len(methods), 1, figsize = (20, 15))

for i, method in enumerate(methods):
    Z = linkage(data_scaled, metric = 'euclidean', method = method)

    dendrogram(Z, ax = axs[i]);

    axs[i].set_title(f'Dendrogram ({method.capitalize()} Linkage)')

    axs[i].set_ylabel('Distance')

plt.figure(figsize = (20, 7))  #creating fig size

plt.title("Dendrograms")  #adding title

dend = dendrogram(linkage(data_pca1, metric = 'euclidean', method = 'average')) #creating dendrogram specifying distance metric and linkage method

plt.axhline(y = 7, color = 'r', linestyle = '--') #specifying where dendrogram needs to be cut

"""**Think about it:**

- Can we clearly decide the number of clusters based on where to cut the dendrogram horizontally?

Yes, we look at the highest point in the dendrogram and make a cut at the value where we could evenly cut the graph.

- What is the next step in obtaining number of clusters based on the dendrogram?

We will see where we can cut the dendrogram, then initialize PCA to visualize the clusters.

- Are there any distinct clusters in any of the dendrograms?

Given the size and scale of the data it's extremely hard to tell if there are any distinct clusters.

### **Visualize the clusters using PCA**
"""

hierarchical = AgglomerativeClustering(n_clusters = 7, affinity = 'euclidean', linkage = 'average')

hierarchical.fit(data_pca1)

df['HCLabels'] = hierarchical.labels_

"""### **Cluster Profiling**"""

plt.figure(figsize = (30, 50))

for i, variable in enumerate(df):
    plt.subplot(8, 3, i + 1)

    sns.boxplot(y=df[variable], x=df["HCLabels"])

    plt.tight_layout()

    plt.title(variable)

plt.show()

"""**Observations and Insights:**

After exploring all linkages and distances, I ultimately decided to fit the data using euclidean distance with average linkage because it had the highest cophenetic correlation. The interesting thing I noticed in the clusters compared to all others was that it categorized all the outliers into their own clusters. In some ways this was helpful because it meant those outliers were grouped together, however it also meant that some of the data had very odd proportions. For instance Cluster 1 regularly had a massive upper quartile range. Overall, it's hard to compare these to kmeans or kmedoids because there are more clusters and there are also two clusters devoted to outliers.

After looking through linkages I noticed that single almost always yielded a blob of green all across the board telling me that the clusters were not robust at all. Average linkage demonstrated some improvement, but they were still close together. Complete linkage seemed to produce the highest clusters, however they also didn't have a high cophenetic correlation. Cityblock distances were super close together until complete linkage, however this would require us to have over 20 clusters. Mahalanobis had a really interesting long tree over to the right on each cluster that seemed curious. Their clusters weren't particularly high though, so they didn't seem like a viable option. Chebyshev had a very spread but not high single linkage cluster. They complete linkage was high, but it would have been hard to decide where to cut the dendrogram. Finally, Euclidean had a great mixture of having high trees, but also having a low number of clusters. Ultimately I decided to use average linkage to explore what the clusters would look like with the highest cophenetic correlation.

### **Characteristics of each cluster**

**Summary of each cluster:**

- Cluster 0: This cluster is upper class, they have an extremely wide range of support for the shop so it’s hard to tell how often they interact with the store. They are primarily middle aged. They’re likely to have 1-2 kids and as a result have larger families. The majority of them have been fierce supporters of the business. They accept a lot of offers. They primarily spend over 1,000 per purchase and their total number of purchases is well above the rest.

- Cluster 1: This cluster is very affluent with a slim income range near 160,000. They have a wide range of support for the store, but they’re mostly frequent shoppers. They are primarily in their late 40s to 50s. They’re likely to have 1-2 kids and as a result have larger families. They are new supporters. They don’t accept offers at all. They spend a very wide range per purchase and the upper quartile has a gigantic range, but the median is somewhere in the double digits. Their number of purchases range drastically so it’s hard to tell where to categorize them here.

- Cluster 2: This cluster is lower to middle class with a wide age range. Their shopping frequency ranges the entire length of the graph, so it’s hard to say how frequently they shop. They have a very wide age range all the way from 20 to 75. They’re likely to have 1-2 kids and as a result have larger families. Their loyalty ranges, so it’s hard to tell how long they’ve been supporters of the store. They accept offers fairly regularly. They don’t spend a lot per purchase, but there are a few who do spend quite a bit. Their total purchase number is in the middle range compared to the others.

- Cluster 3: The model seems to have classified the one individual with 666,666 as its own cluster, so this cluster seems to only represent them. This one shopper seems to not shop at the store frequently. This shopper appears to be 39. This person appears to have one child and as a result has a family of three. They appear to have supported the store for three years. They don’t accept offers at all. They spend in the double digits every purchase, however since this is just 1 person, it’s hard to imagine this being any higher unless they were consistently spending a lot on each purchase. Their total purchase number is around 12.

- Cluster 4: This cluster is primarily upper middle class. They seem to shop fairly frequently at the store. This cluster appears to only be made up of the 3 individuals that somehow managed to make it past 100 and still shop at the store. They’re likely to have two kids and as a result have a family of four. They are new supporters. They rarely accept offers. They don’t spend a lot per purchase, but there are a few who do spend quite a bit. Their total purchase number is relatively low compared to the rest.

- Cluster 5: This cluster is middle class but with a wide range that spans up to 100,000. Their shopping frequency ranges the entire length of the graph, so it’s hard to say how frequently they shop. They are middle aged to older. They’re likely to have several kids and as a result have larger families. They have a wide range of support, but are primarily new shoppers. They accept offers fairly regularly. They don’t spend a lot per purchase, but there are a few who do spend quite a bit. Their total purchase number is in the middle range compared to the others.

- Cluster 6: This cluster is primarily upper class. Their shopping frequency ranges the entire length of the graph, so it’s hard to say how frequently they shop. They have a very wide age range all the way from 20 to 75. They aren’t likely to have kids, and as a result have smaller families. They have a wide range of support, but are primarily new shoppers. They accept a lot of offers. They primarily spend over 1,700 per purchase. They spend the most amount per purchase compared to any cluster. They primarily spend over 1,000 per purchase and their total number of purchases is well above the rest.

## **DBSCAN**

DBSCAN is a very powerful algorithm for finding high-density clusters, but the problem is determining the best set of hyperparameters to use with it. It includes two hyperparameters, `eps`, and `min samples`.

Since it is an unsupervised algorithm, you have no control over it, unlike a supervised learning algorithm, which allows you to test your algorithm on a validation set. The approach we can follow is basically trying out a bunch of different combinations of values and finding the silhouette score for each of them.
"""

df.drop(columns = "HCLabels", inplace = True)

"""### **Apply DBSCAN for the best hyperparameter and visualize the clusters from PCA**"""

eps_value = [1,5]                       #Inputting eps value
min_sample_values = [10,20]              #Inputting min_sample value


res = {eps_value[i]: min_sample_values for i in range(len(eps_value))} #Creating a dictionary for all eps_value and min_sample_values

# Finding the silhouette_score for each of the combinations
high_silhouette_avg = 0                                               # Creating high_silhouette_avg variable
high_i_j = [0, 0]                                                     # Creating high_i_j list
key = res.keys()                                                      # Assigning dictionary keys to a variable called key
for i in key:
    z = res[i]                                                        # Assigning dictionary values of each i to z
    for j in z:
        db = DBSCAN(eps=i, min_samples=j).fit(data_pca1.values)               # Applying DBSCAN to each combination in dictionary
        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
        core_samples_mask[db.core_sample_indices_] = True
        labels = db.labels_
        silhouette_avg = silhouette_score(data_pca1.values, labels)           # Calculating silhouette score
        print(
            "For eps value =" + str(i),
            "For min sample =" + str(j),
            "The average silhoutte_score is :",
            silhouette_avg,                                          # Printing the silhouette score for each of the combinations
        )
        if high_silhouette_avg < silhouette_avg:                     # If the silhouette score is greater than 0 or the previous score, it will get appended to the high_silhouette_avg list with its combination of i and j
            high_i_j[0] = i
            high_i_j[1] = j

dbs = DBSCAN(eps = 1, min_samples= 20) #initializing DBSCAN with 1 eps and 20 min samples

df['DBSLabels'] = dbs.fit_predict(data_pca1.values) #fitting and predicting DBS to data

plt.figure(figsize = (30, 50))

for i, variable in enumerate(df):
    plt.subplot(8, 3, i + 1)

    sns.boxplot(y=df[variable], x=df['DBSLabels'])

    plt.tight_layout()

    plt.title(variable)

plt.show()

"""**Observations and Insights:**

DBSCAN was interesting to work with because it was just trial and error. I used several differnt values for the eps and min values in an effort to find the highest silhouette score, but I was unable to get anything that approached a .9. I was interested to find that the silhouette scores often all turned out to match up, so I tried to keep plugging and running values until I could get different scores which is why I ultimately settled on the values above. Additionally, when I ran any eps score higher than 1 I only got two clusters which is not representative of our data, so I decided to run us 1 and 20 as this was also extremely close to 1. Additionally, for some reason the model has classified one of the clusters as -1. This I was confused about because none of the other models seemed to do this.

**Think about it:**

- Changing the eps and min sample values will result in different DBSCAN results? Can we try more value for eps and min_sample?

They do give us different results because they compute different silhouette scores. The results aren't drastic by any means, but they will still differ depending on the eps and min values. I tried multiple scores in pursuit of the highest silhouette score but was unable to reach anything that approached a .9. Ultimately this is just a process of trial and error.

### **Characteristics of each cluster**

**Summary of each cluster:**

- Cluster -1: This cluster has a wide class range, they have an extremely wide range of support for the shop so it’s hard to tell how often they interact with the store. They are primarily middle aged. They’re likely to have 1-2 kids and as a result have larger families. Their support is all over the place. It spans all years. They accept a lot of offers. They primarily spend over 500 per purchase, but aren’t shy about spending more, and their total number of purchases is well above the rest.

- Cluster 0: This cluster is primarily middle class. They are mostly frequent shoppers. They are primarily in their late 40s to 50s. They’re likely to have 1-2 kids and as a result have larger families. They have been supporters for three years. They don’t accept offers at all. They spend well under 500 per purchase and their number of purchases range but it’s under 10.

- Cluster 1: This cluster is lower to middle class and primarily under 40. Their shopping frequency is in the upper half of the graph, so this tells me they circuit the store frequently. They’re likely to have 1-2 kids but seem to have smaller families. They have been supporters for three years. They don’t accept offers at all. They spend well under 500 per purchase and their number of purchases range but it’s under 10.

- Cluster 2: This cluster is lower to middle class and primarily in their 40. Their shopping frequency varies greatly as it looks like it spans the whole graph. They’re likely to have 1-2 kids and seem to have larger families. They are new supporters of the store. They don’t accept offers at all. They spend well under 500 per purchase and their number of purchases range but it’s under 10.

- Cluster 3: This cluster is primarily middle class. They seem to shop very fairly frequently at the store. They seem to be middle aged with a median age of 50. They’re likely to have two kids and as a result have a family of four. They have been supporters for three years. They don’t accept offers at all. They spend well under 500 per purchase and their number of purchases range, normally it’s under 10, but they do seem to out purchase some of the other clusters.

## **Gaussian Mixture Model**
"""

df.drop(columns = 'DBSLabels', inplace = True)

gmm = GaussianMixture(n_components = 5, random_state = 1) #initializing GMM with 5 components

gmm.fit(data_pca1.values) #fitting data

df['GmmLabels'] = gmm.predict(data_pca1.values) #adding labels to data

silhouette_avg = silhouette_score(df.values, labels) #calculating silhouette score and printing the score
print('Silhouette Score is:', silhouette_avg)

"""**Observations and Insights:**

For only using 5 clusters .96 is an excellent silhouette score. This is possibly the closest to 1 we will get. I tried a couple of lower numbers in the components and still got the same result, this tells me that GMM is a much more reliable method to use than DBSCAN since those silhouette scores were in the lower .8 range.

### **Visualize the clusters using PCA**

### **Cluster Profiling**
"""

plt.figure(figsize = (30, 50))

for i, variable in enumerate(df):
    plt.subplot(8, 3, i + 1)

    sns.boxplot(y=df[variable], x=df["GmmLabels"])

    plt.tight_layout()

    plt.title(variable)

plt.show()

"""### **Characteristics of each cluster**

**Summary of each cluster:**

Cluster 0: This cluster is primarily upper middle class. They are mostly frequent shoppers. They are primarily in their late 50s. They’re likely to have 1-2 kids and as a result have larger families. They are loyal supporters. They sometimes accept offers. They spend on average close to 1000 per purchase and their number of purchases range but it’s over 20.

Cluster 1: This cluster is middle class and closer to their 40s. Their shopping frequency is somewhere in the mid-range so they’re casual shoppers. They’re likely to have 1 kid and seem to have smaller families. They have been supporters for three years. They accept a lot of offers. They spend well under 500 per purchase and their number of purchases range but it’s normally above 10.

Cluster 2: This cluster is upper middle class and their age range varies greatly. Their shopping frequency varies greatly as it looks like it spans the whole graph. They're unlikely to have any kids, so they have smaller families. They have been supporters of the store for three years. They accept a lot of offers. They normally spend over 500 per purchase and have made upwards of 20 purchases.

Cluster 3: This cluster is primarily middle class. They seem to shop very fairly frequently at the store. They seem to be middle aged with a median age of 50. They’re likely to have two kids and as a result have a family of four. They are fierce supporters. They don’t accept offers at all. They normally spend over 1,000 per purchase and their number of purchases seems to be somewhere around 20.

Cluster 4: This cluster is very affluent. They don’t seem to shop very fairly frequently at the store. They seem to be in their late 40s to middle aged. They’re likely to have 1-2 kids and as a result have larger families. They are casual to very loyal supporters. They accept a lot of offers. They spend well above 1000 per purchase and their number of purchases range, but normally it’s
around 20.

## **Conclusion and Recommendations**

**1. Comparison of various techniques and their relative performance based on chosen Metric (Measure of success)**:
- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?

All of the models ended up having the same issues and similar results, however they classified clusters differently in the sense that cluster 0 in one model could be lower class with children, but in the next model they were upper class with no kids. I think the metric of success for me was how refined was the distribution of each box plot, because this told me that the clusters were giving me a good analysis of the data. The model that I think performed the worst was DBSCAN because there were so many variables that had distributions with completely flat box plots. This tells us nothing and is useless to the client. There are some variables with low amounts of data e.g. total kids in home where the number only goes so high, but others like length of support were also completely flat. Hierarchical clustering was a lot of fun to toy with from a data scientist perspective, but I can’t see selling a client on this due to how technically involved it is. Gaussian Mixture model didn’t have poor performance, however so many of the distributions on the box plots were so big that it didn’t tell us much about the data so it wouldn’t be useful to the client. In my opinion, for this stakeholder it would come down to kmeans and kmedoids. Kmedoids did an excellent job at providing varying distributions of the data which tells me we’re getting more refined clusters, kmeans also provided pretty good distributions in the box plots, however some of the distributions looked to be on either the high or the low side of the graph. This tells me that the clusters need to be further refined. I think some of the other models above could be improved by simply providing more data. For instance DBSCAN will most likely perform better with a larger data set, as would GMM. As for hierarchical clustering, I wonder if a better solution would be to spend more time maybe refining my variables. This would look like maybe creating other variables out of the existing data set and dropping things like family size length of support that have low numbers.

**2. Refined insights**:
- What are the most meaningful insights from the data relevant to the problem?

In my opinion some of the most interesting insights are that the customer base is primarily filled with new supporters, they’re mostly middle to upper class, mostly middle aged to older, with some outliers, a lot of them have kids, a good majority of them have interacted with the store frequently in the past 60 days which is a positive sign, and most of them are spending under 1,000 per purchase, but they’re still pretty frequent shoppers. The number one interesting fact I found though was that no one seems to be accepting the offers. There are a few who are, but it seems like those are mainly the people without kids which means that the store might have to do more work to target people in store more or work to refine how they send out their offers.

**3. Proposal for the final solution design:**
- What model do you propose to be adopted? Why is this the best solution to adopt?

My recommendation here is to implement the K-medoids model for further analysis of their customer base. Out of all the models, this one yielded by far the best distributions. I didn’t see as many box plots that were just flat. And there was also only one box plot that was dead center in the graph and spanned the whole graph. This tells me that this model is giving us the most refined and robust clusters for our data and will ultimately help our stakeholders segment their customer base successfully. Additionally, kmedoids is less sensitive to outliers compared to the other models. This dataset had a couple of odd outliers in the age and income column that threw some of the other models for a loop, so I think this would be the best model to help combat those odd outliers that make their way into the data. This has the potential to drive revenue substantially for the stakeholders. Their customer base is not interacting with the offers. If they are able to target their customer base more, they may see a higer target capture rate and more traffic in the store, thus driving revenue. However, this comes at a high cost of developing the model (this can range from $60k-$100k), the maintnence of the model, and the continual uploading of data into the model. More data must be collected from their customer base to determine if this is trule the best model. Going off of what we have now, I am comfortable saying it is, but this is still a relatively small dataset. Additionally, the stakeholder needs to see if this is a viable solution for them financially and if they can truly afford it.
"""

